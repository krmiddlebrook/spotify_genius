{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6988, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             song  year           artist genre  \\\n",
       "0      0        ego-remix  2009  beyonce-knowles   Pop   \n",
       "1      1     then-tell-me  2009  beyonce-knowles   Pop   \n",
       "2      2          honesty  2009  beyonce-knowles   Pop   \n",
       "3      3  you-are-my-rock  2009  beyonce-knowles   Pop   \n",
       "4      4    black-culture  2009  beyonce-knowles   Pop   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
       "1  playin' everything so easy,\\nit's like you see...  \n",
       "2  If you search\\nFor tenderness\\nIt isn't hard t...  \n",
       "3  Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
       "4  Party the people, the people the party it's po...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs = pd.read_csv('/Users/kaimiddlebrook/Downloads/lyrics.csv')\n",
    "songs = songs.head(10000)\n",
    "songs = songs.dropna()\n",
    "print(songs.shape)\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pop', 'Hip-Hop', 'Not Available', 'Rock', 'Metal', 'Other',\n",
       "       'Country', 'Jazz', 'Electronic', 'Folk', 'R&B', 'Indie'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oh baby, how you doing? You know Im gonna cut right to the chase Some women '\n",
      " 'were made but me, myself I like to think that I was created for a special '\n",
      " 'purpose You know, whats more special than you? You feel me Its on baby, lets '\n",
      " 'get lost You dont need to call into work cause youre the boss For real, want '\n",
      " 'you to show me how you feel I consider myself lucky, thats a big deal Why? '\n",
      " 'Well, you got the key to my heart But you aint gonna need it, Id rather you '\n",
      " 'open up my body And show me secrets, you didnt know was inside No need for '\n",
      " 'me to lie Its too big, its too wide Its too strong, it wont fit Its too '\n",
      " 'much, its too tough He talk like this cause he can back it up He got a big '\n",
      " 'ego, such a huge ego I love his big ego, its too much He walk like this '\n",
      " 'cause he can back it up Usually Im humble, right now I dont choose You can '\n",
      " 'leave with me or you could have the blues Some call it arrogant, I call it '\n",
      " 'confident You decide when you find on what Im working with Damn I know Im '\n",
      " 'killing you with them legs Better yet them thighs Matter a fact its my smile '\n",
      " 'or maybe my eyes Boy you a site to see, kind of something like me Its too '\n",
      " 'big, its too wide Its too strong, it wont fit Its too much, its too tough I '\n",
      " 'talk like this cause I can back it up I got a big ego, such a huge ego But '\n",
      " 'he love my big ego, its too much I walk like this cause I can back it up I, '\n",
      " 'I walk like this cause I can back it up I, I talk like this cause I can back '\n",
      " 'it up I, I can back it up, I can back it up I walk like this cause I can '\n",
      " 'back it up Its too big, its too wide Its too strong, it wont fit Its too '\n",
      " 'much, its too tough He talk like this cause he can back it up He got a big '\n",
      " 'ego, such a huge ego, such a huge ego I love his big ego, its too much He '\n",
      " 'walk like this cause he can back it up Ego so big, you must admit I got '\n",
      " 'every reason to feel like Im that bitch Ego so strong, if you aint know I '\n",
      " 'dont need no beat, I can sing it with piano']\n"
     ]
    }
   ],
   "source": [
    "lyrics = songs.lyrics.values.tolist()\n",
    "lyrics = [re.sub(r'\\s+', ' ', line) for line in lyrics] # remove whitespace and newlines\n",
    "lyrics = [re.sub(\"\\'\", \"\", line) for line in lyrics] # remove distracting single quotes\n",
    "pprint(lyrics[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words and Clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['oh', 'baby', 'how', 'you', 'doing', 'you', 'know', 'im', 'gonna', 'cut', 'right', 'to', 'the', 'chase', 'some', 'women', 'were', 'made', 'but', 'me', 'myself', 'like', 'to', 'think', 'that', 'was', 'created', 'for', 'special', 'purpose', 'you', 'know', 'whats', 'more', 'special', 'than', 'you', 'you', 'feel', 'me', 'its', 'on', 'baby', 'lets', 'get', 'lost', 'you', 'dont', 'need', 'to', 'call', 'into', 'work', 'cause', 'youre', 'the', 'boss', 'for', 'real', 'want', 'you', 'to', 'show', 'me', 'how', 'you', 'feel', 'consider', 'myself', 'lucky', 'thats', 'big', 'deal', 'why', 'well', 'you', 'got', 'the', 'key', 'to', 'my', 'heart', 'but', 'you', 'aint', 'gonna', 'need', 'it', 'id', 'rather', 'you', 'open', 'up', 'my', 'body', 'and', 'show', 'me', 'secrets', 'you', 'didnt', 'know', 'was', 'inside', 'no', 'need', 'for', 'me', 'to', 'lie', 'its', 'too', 'big', 'its', 'too', 'wide', 'its', 'too', 'strong', 'it', 'wont', 'fit', 'its', 'too', 'much', 'its', 'too', 'tough', 'he', 'talk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'he', 'got', 'big', 'ego', 'such', 'huge', 'ego', 'love', 'his', 'big', 'ego', 'its', 'too', 'much', 'he', 'walk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'usually', 'im', 'humble', 'right', 'now', 'dont', 'choose', 'you', 'can', 'leave', 'with', 'me', 'or', 'you', 'could', 'have', 'the', 'blues', 'some', 'call', 'it', 'arrogant', 'call', 'it', 'confident', 'you', 'decide', 'when', 'you', 'find', 'on', 'what', 'im', 'working', 'with', 'damn', 'know', 'im', 'killing', 'you', 'with', 'them', 'legs', 'better', 'yet', 'them', 'thighs', 'matter', 'fact', 'its', 'my', 'smile', 'or', 'maybe', 'my', 'eyes', 'boy', 'you', 'site', 'to', 'see', 'kind', 'of', 'something', 'like', 'me', 'its', 'too', 'big', 'its', 'too', 'wide', 'its', 'too', 'strong', 'it', 'wont', 'fit', 'its', 'too', 'much', 'its', 'too', 'tough', 'talk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'got', 'big', 'ego', 'such', 'huge', 'ego', 'but', 'he', 'love', 'my', 'big', 'ego', 'its', 'too', 'much', 'walk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'walk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'talk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'can', 'back', 'it', 'up', 'can', 'back', 'it', 'up', 'walk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'its', 'too', 'big', 'its', 'too', 'wide', 'its', 'too', 'strong', 'it', 'wont', 'fit', 'its', 'too', 'much', 'its', 'too', 'tough', 'he', 'talk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'he', 'got', 'big', 'ego', 'such', 'huge', 'ego', 'such', 'huge', 'ego', 'love', 'his', 'big', 'ego', 'its', 'too', 'much', 'he', 'walk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'ego', 'so', 'big', 'you', 'must', 'admit', 'got', 'every', 'reason', 'to', 'feel', 'like', 'im', 'that', 'bitch', 'ego', 'so', 'strong', 'if', 'you', 'aint', 'know', 'dont', 'need', 'no', 'beat', 'can', 'sing', 'it', 'with', 'piano']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(lyrics))\n",
    "print(data_words[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bigrams and Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'baby', 'how', 'you', 'doing', 'you', 'know', 'im', 'gonna', 'cut', 'right', 'to', 'the', 'chase', 'some', 'women', 'were', 'made', 'but', 'me', 'myself', 'like', 'to', 'think', 'that', 'was', 'created', 'for', 'special', 'purpose', 'you', 'know', 'whats', 'more', 'special', 'than', 'you', 'you', 'feel', 'me', 'its', 'on', 'baby', 'lets', 'get', 'lost', 'you', 'dont', 'need', 'to', 'call', 'into', 'work', 'cause', 'youre', 'the', 'boss', 'for', 'real', 'want', 'you', 'to', 'show', 'me', 'how', 'you', 'feel', 'consider', 'myself', 'lucky', 'thats', 'big_deal', 'why', 'well', 'you', 'got', 'the', 'key', 'to', 'my', 'heart', 'but', 'you', 'aint', 'gonna', 'need', 'it', 'id_rather', 'you', 'open', 'up', 'my', 'body', 'and', 'show', 'me', 'secrets', 'you', 'didnt', 'know', 'was', 'inside', 'no', 'need', 'for', 'me', 'to', 'lie', 'its', 'too', 'big', 'its', 'too', 'wide', 'its', 'too', 'strong', 'it', 'wont', 'fit', 'its', 'too', 'much', 'its', 'too', 'tough', 'he', 'talk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'he', 'got', 'big_ego_such_huge', 'ego', 'love', 'his', 'big_ego', 'its', 'too', 'much', 'he', 'walk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'usually', 'im', 'humble', 'right', 'now', 'dont', 'choose', 'you', 'can', 'leave', 'with', 'me', 'or', 'you', 'could', 'have', 'the', 'blues', 'some', 'call', 'it', 'arrogant', 'call', 'it', 'confident', 'you', 'decide', 'when', 'you', 'find', 'on', 'what', 'im', 'working', 'with', 'damn', 'know', 'im', 'killing', 'you', 'with', 'them', 'legs', 'better', 'yet', 'them', 'thighs', 'matter_fact', 'its', 'my', 'smile', 'or', 'maybe', 'my', 'eyes', 'boy', 'you', 'site', 'to', 'see', 'kind', 'of', 'something', 'like', 'me', 'its', 'too', 'big', 'its', 'too', 'wide', 'its', 'too', 'strong', 'it', 'wont', 'fit', 'its', 'too', 'much', 'its', 'too', 'tough', 'talk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'got', 'big_ego_such_huge', 'ego', 'but', 'he', 'love', 'my', 'big_ego', 'its', 'too', 'much', 'walk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'walk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'talk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'can', 'back', 'it', 'up', 'can', 'back', 'it', 'up', 'walk', 'like', 'this', 'cause', 'can', 'back', 'it', 'up', 'its', 'too', 'big', 'its', 'too', 'wide', 'its', 'too', 'strong', 'it', 'wont', 'fit', 'its', 'too', 'much', 'its', 'too', 'tough', 'he', 'talk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'he', 'got', 'big_ego_such_huge', 'ego', 'such_huge_ego', 'love', 'his', 'big_ego', 'its', 'too', 'much', 'he', 'walk', 'like', 'this', 'cause', 'he', 'can', 'back', 'it', 'up', 'ego', 'so', 'big', 'you', 'must', 'admit', 'got', 'every', 'reason', 'to', 'feel', 'like', 'im', 'that', 'bitch', 'ego', 'so', 'strong', 'if', 'you', 'aint', 'know', 'dont', 'need', 'no', 'beat', 'can', 'sing', 'it', 'with', 'piano']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['baby', 'how', 'do', 'know', 'be', 'go', 'cut', 'right', 'chase', 'woman', 'be', 'make', 'like', 'think', 'be', 'create', 'special', 'purpose', 'know', 's', 'more', 'special', 'feel', 'baby', 'let', 'get', 'lose', 'do', 'not', 'need', 'call', 'work', 'be', 'boss', 'real', 'want', 'show', 'how', 'feel', 'consider', 'lucky', 's', 'big_deal', 'why', 'get', 'key', 'heart', 'be', 'not', 'go', 'need', 'open', 'body', 'show', 'secret', 'do', 'not', 'know', 'be', 'need', 'lie', 'too', 'big', 'too', 'wide', 'too', 'strong', 'not', 'fit', 'too', 'much', 'too', 'tough', 'talk', 'can', 'back', 'get', 'big_ego', 'such_huge', 'ego', 'love', 'too', 'much', 'walk', 'can', 'back', 'usually', 'be', 'humble', 'right', 'now', 'do', 'not', 'choose', 'can', 'leave', 'could', 'have', 'blue', 'call', 'arrogant', 'call', 'confident', 'decide', 'when', 'find', 'be', 'work', 'damn', 'know', 'be', 'kill', 'leg', 'better', 'yet', 'thigh', 'matter_fact', 'smile', 'maybe', 'eye', 'boy', 'site', 'see', 'kind', 'something', 'too', 'big', 'too', 'wide', 'too', 'strong', 'not', 'fit', 'too', 'much', 'too', 'tough', 'talk', 'can', 'back', 'get', 'big_ego', 'such_huge', 'ego', 'love', 'too', 'much', 'walk', 'can', 'back', 'walk', 'can', 'back', 'talk', 'can', 'back', 'can', 'back', 'can', 'back', 'walk', 'can', 'back', 'too', 'big', 'too', 'wide', 'too', 'strong', 'not', 'fit', 'too', 'much', 'too', 'tough', 'talk', 'can', 'back', 'get', 'big_ego', 'such_huge', 'ego', 'such_huge', 'ego', 'love', 'too', 'much', 'walk', 'can', 'back', 'ego', 'so', 'big', 'must', 'admit', 'get', 'reason', 'feel', 'be', 'bitch', 'ego', 'so', 'strong', 'be', 'not', 'know', 'do', 'not', 'need', 'beat', 'can', 'sing', 'piano']]\n"
     ]
    }
   ],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 2), (3, 11), (4, 11), (5, 1), (6, 1), (7, 4), (8, 1), (9, 3), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 3), (16, 13), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 5), (27, 6), (28, 1), (29, 3), (30, 1), (31, 3), (32, 6), (33, 2), (34, 1), (35, 1), (36, 2), (37, 1), (38, 1), (39, 1), (40, 1), (41, 5), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 6), (55, 1), (56, 4), (57, 9), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 2), (65, 2), (66, 1), (67, 1), (68, 2), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 2), (75, 4), (76, 4), (77, 4), (78, 1), (79, 1), (80, 18), (81, 3), (82, 1), (83, 5), (84, 1), (85, 1), (86, 1), (87, 3), (88, 1), (89, 2), (90, 1)]]\n",
      "admit\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[:1]) # view (word_id, word_frequency) for the first document\n",
    "print(id2word[0]) # view the word that corresponds to word id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('admit', 1),\n",
       "  ('arrogant', 1),\n",
       "  ('baby', 2),\n",
       "  ('back', 11),\n",
       "  ('be', 11),\n",
       "  ('beat', 1),\n",
       "  ('better', 1),\n",
       "  ('big', 4),\n",
       "  ('big_deal', 1),\n",
       "  ('big_ego', 3),\n",
       "  ('bitch', 1),\n",
       "  ('blue', 1),\n",
       "  ('body', 1),\n",
       "  ('boss', 1),\n",
       "  ('boy', 1),\n",
       "  ('call', 3),\n",
       "  ('can', 13),\n",
       "  ('chase', 1),\n",
       "  ('choose', 1),\n",
       "  ('confident', 1),\n",
       "  ('consider', 1),\n",
       "  ('could', 1),\n",
       "  ('create', 1),\n",
       "  ('cut', 1),\n",
       "  ('damn', 1),\n",
       "  ('decide', 1),\n",
       "  ('do', 5),\n",
       "  ('ego', 6),\n",
       "  ('eye', 1),\n",
       "  ('feel', 3),\n",
       "  ('find', 1),\n",
       "  ('fit', 3),\n",
       "  ('get', 6),\n",
       "  ('go', 2),\n",
       "  ('have', 1),\n",
       "  ('heart', 1),\n",
       "  ('how', 2),\n",
       "  ('humble', 1),\n",
       "  ('key', 1),\n",
       "  ('kill', 1),\n",
       "  ('kind', 1),\n",
       "  ('know', 5),\n",
       "  ('leave', 1),\n",
       "  ('leg', 1),\n",
       "  ('let', 1),\n",
       "  ('lie', 1),\n",
       "  ('like', 1),\n",
       "  ('lose', 1),\n",
       "  ('love', 3),\n",
       "  ('lucky', 1),\n",
       "  ('make', 1),\n",
       "  ('matter_fact', 1),\n",
       "  ('maybe', 1),\n",
       "  ('more', 1),\n",
       "  ('much', 6),\n",
       "  ('must', 1),\n",
       "  ('need', 4),\n",
       "  ('not', 9),\n",
       "  ('now', 1),\n",
       "  ('open', 1),\n",
       "  ('piano', 1),\n",
       "  ('purpose', 1),\n",
       "  ('real', 1),\n",
       "  ('reason', 1),\n",
       "  ('right', 2),\n",
       "  ('s', 2),\n",
       "  ('secret', 1),\n",
       "  ('see', 1),\n",
       "  ('show', 2),\n",
       "  ('sing', 1),\n",
       "  ('site', 1),\n",
       "  ('smile', 1),\n",
       "  ('so', 2),\n",
       "  ('something', 1),\n",
       "  ('special', 2),\n",
       "  ('strong', 4),\n",
       "  ('such_huge', 4),\n",
       "  ('talk', 4),\n",
       "  ('thigh', 1),\n",
       "  ('think', 1),\n",
       "  ('too', 18),\n",
       "  ('tough', 3),\n",
       "  ('usually', 1),\n",
       "  ('walk', 5),\n",
       "  ('want', 1),\n",
       "  ('when', 1),\n",
       "  ('why', 1),\n",
       "  ('wide', 3),\n",
       "  ('woman', 1),\n",
       "  ('work', 2),\n",
       "  ('yet', 1)]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the topics in LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.091*\"win\" + 0.033*\"mile\" + 0.020*\"above\" + 0.018*\"thin\" + 0.016*\"course\" '\n",
      "  '+ 0.011*\"press\" + 0.011*\"cheek\" + 0.009*\"boom_boom\" + 0.009*\"pas\" + '\n",
      "  '0.009*\"believer\"'),\n",
      " (1,\n",
      "  '0.106*\"be\" + 0.056*\"not\" + 0.039*\"do\" + 0.021*\"go\" + 0.019*\"know\" + '\n",
      "  '0.017*\"have\" + 0.014*\"love\" + 0.014*\"get\" + 0.014*\"so\" + 0.014*\"s\"'),\n",
      " (2,\n",
      "  '0.045*\"hot\" + 0.038*\"shadow\" + 0.033*\"heat\" + 0.032*\"warrior\" + '\n",
      "  '0.019*\"crowd\" + 0.016*\"thunder\" + 0.016*\"daylight\" + 0.016*\"race\" + '\n",
      "  '0.015*\"freak\" + 0.013*\"drown\"'),\n",
      " (3,\n",
      "  '0.047*\"moon\" + 0.041*\"summer\" + 0.027*\"remain\" + 0.020*\"dem\" + '\n",
      "  '0.019*\"beautiful\" + 0.019*\"prepare\" + 0.016*\"flower\" + 0.015*\"letter\" + '\n",
      "  '0.015*\"wound\" + 0.015*\"wan\"'),\n",
      " (4,\n",
      "  '0.073*\"shorty\" + 0.059*\"whip\" + 0.055*\"dance\" + 0.028*\"ghetto\" + '\n",
      "  '0.022*\"wrist\" + 0.015*\"thinking\" + 0.015*\"body\" + 0.014*\"mood\" + '\n",
      "  '0.013*\"party\" + 0.012*\"your\"'),\n",
      " (5,\n",
      "  '0.016*\"hat\" + 0.015*\"quiero\" + 0.013*\"ich\" + 0.012*\"und\" + 0.012*\"son\" + '\n",
      "  '0.011*\"sei\" + 0.011*\"doch\" + 0.011*\"con\" + 0.009*\"der\" + 0.009*\"jetzt\"'),\n",
      " (6,\n",
      "  '0.048*\"be\" + 0.024*\"will\" + 0.012*\"life\" + 0.011*\"have\" + 0.011*\"light\" + '\n",
      "  '0.011*\"come\" + 0.010*\"time\" + 0.010*\"now\" + 0.009*\"there\" + 0.009*\"day\"'),\n",
      " (7,\n",
      "  '0.064*\"get\" + 0.035*\"nigga\" + 0.021*\"shit\" + 0.018*\"man\" + 0.017*\"be\" + '\n",
      "  '0.016*\"fuck\" + 0.015*\"bitch\" + 0.013*\"money\" + 0.013*\"niggas\" + '\n",
      "  '0.009*\"hit\"'),\n",
      " (8,\n",
      "  '0.069*\"que\" + 0.021*\"te\" + 0.017*\"yo\" + 0.013*\"mas\" + 0.013*\"el\" + '\n",
      "  '0.012*\"por\" + 0.011*\"sin\" + 0.010*\"todo\" + 0.010*\"como\" + 0.009*\"vida\"'),\n",
      " (9,\n",
      "  '0.021*\"don\" + 0.016*\"dear\" + 0.016*\"qui\" + 0.013*\"inch\" + 0.011*\"anche\" + '\n",
      "  '0.009*\"sie\" + 0.008*\"noch\" + 0.008*\"die\" + 0.008*\"open_wide\" + '\n",
      "  '0.008*\"pool\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
